---
title: 机器学习
author: lonelywatch
date: 2024-9-10 21:28 +0800
categories: [ML]
tags: [ML] 
---

# 机器学习

机器学习指的是计算机不经过显式编程而具有学习能力，是人工智能的一个分支，其算法可以通过数据训练来实现。通常基于概率论，统计学，线性代数等数学知识。

> 后续所有内容来自《机器学习》，此处仅为总结与归纳。

机器学习基于给定数据集进行学习，数据集中的计量单位称为样本，样本的属性称为特征，样本的属性与特征的对应关系称为标签，同时所有的样本组成了样本空间或者称为特征空间。对于一数据集，所有存在的假设集合被称为假设空间，其中所有符合规律的假设集合组成了版本空间，机器学习的目标是针对某一特定问题，来寻找假设，使得假设对应该问题背后蕴含的规律，这一过程就被称为学习。

如果版本空间中存在多种假设，则考虑偏好函数，偏好函数是对假设的偏好程度的量化，通常使用损失函数来衡量偏好函数。损失函数是对假设的评价函数，通常使用交叉熵，均方误差等函数来衡量损失函数，通过最小化对应的损失函数值来选择最优的假设。

根据数据集有无标签，可将机器学习分为监督学习与无监督学习。无监督学习主要包含聚类，也就是Cluster，用于尝试寻找数据集中暗含的规律；有监督学习主要针对模型输出值是否连续，可分为回归（输出值连续）与分类。分类又分为二分类与多分类。

## 模型评估与选择

首先了解几个概念：

- 错误率： 分类错误样本数占总样本数的比例

- 精度：分类正确样本数占总样本数的比例

- 训练误差： 训练集上的误差

- 泛化误差：模型在新样本上的误差

- 过拟合：模型在训练集上表现很好，但在测试集上表现很差，可能把训练样本本身特点当作了所有潜在样本的特点

- 欠拟合：模型在训练集上表现很差，可能模型过于简单，对于训练样本的一般性质尚未学好

通过对候选模型的泛化误差进行评估，选择泛化误差最小的模型。常用的评估方法有留出法，交叉验证法，自助法。

通常使用测试集来测试 测试误差作为泛化误差的近似，假设测试集样本从样本真实分布中独立同分布采样，并且不应出现在训练集中。

### 留出法

将数据集划分为两互斥集合，一个用于训练，一个用于测试。通常训练集占总数据集的2/3~4/5，测试集占10%~30%。分配时需要尽可能保持数据分布一致性，为了实现这一目标在划分数据集时通常使用分层抽样。为了提高评估的稳定性，一般需要采用若干次随机划分，并重复实验取平均值。

### 交叉验证法

将数据集划分为k个大小相似的互斥子集，每个子集保持数据分布一致性。每次训练时使用k-1个子集的并集作为训练集，余下的子集作为测试集，这样可以进行k次训练，通过计算k次测试结果的平均值来评估模型性能。需要注意该方法的稳定性通常取决于k的取值，所以该方法通常也被称为**k折交叉验证**。k最常用的取值为10。

和留出法类似，交叉验证法也需要重复实验取最终平均值，以提高评估的稳定性。

在此基础上特殊的有留一法，也就是k=n，n为数据集大小，这样可以最大程度的利用数据集，但是计算量较大。

### 自助法

以自助采样为基础，对于给定容量为m的数据集D,每次随机从D中挑选一个样本，拷贝放入D1中，并将该样本放回原始数据集，重复m次得到D1。初始数据集中约有36.8%左右的样本未出现在采样数据集中，所以可用D1作为训练集，D-D1作为测试集。自助法的优势在于可以从初始数据集中产生多个不同的训练集，这样可以用于评估模型的稳定性。

当初始数据量足够时，留出法和交叉验证法更常用，因为自助法产生的数据集和初始数据集的分布不同，可能会引入估计偏差，但是当数据量较小时，自助法更常用。

### 调参与性能度量

常用的性能度量指标主要有：

#### 均方误差 MSE

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2
$$

#### 错误率 ER

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}I(f(x_i)\neq y_i)
$$

#### 精度 ACC

$$
ACC(f;D)=\frac{1}{m}\sum_{i=1}^{m}I(f(x_i)=y_i)
$$


## 线性模型

线性模型是指通过对样本属性的线性组合来作为模型的输出值，线性模型的形式化定义如下：

$$
f(x)=w^Tx+b
$$

### 线性回归

对于离散属性，如果存在序关系，则通过连续化将其转化为连续值，对于无序属性，通常使用one-hot编码。根据之前提到的线性模型定义，线性回归的目标是找到一组参数w，b，使得模型的输出值与真实值之间的均方误差最小。通常使用RME作为损失函数，基于该方法来求解的方法被称为**最小二乘法**，使得所有样本到直线上的欧式距离之和最短。

求解这一过程被称为最小二乘参数估计。分别对RME求关于w,b的偏导求得极值点，进而得到最优解。

基于线性回归，如果模型中存在多个特征，可以使用多元线性回归，如果模型预测值并不为线性组合，则可进一步表示为：

$$
y = g^{-1}(w^Tx+b)
$$

该模型被称为 **广义线性模型**，其中g称为**联系函数**，通常使用指数函数，对数函数等。

对于分类任务，需要将模型输出值进行映射。通常使用某种单调可微的函数，比如sigmoid函数，将分类时为正类的可能性与作为反类的可能性之比称为**几率**。将几率取对数得到**对数几率**，对数几率的分类模型称为**对数几率回归**。

### 线性判别分析

LDA也被称为Fisher判别分析。其思想为：将所有样例投影到一条直线上，使得同类样例的投影点尽可能接近，不同类样例的投影点尽可能远离。

### 多分类

## 决策树

## 神经网络