---
title: Transformer
author: lonelywatch
date: 2025-1-7 1:28 +0800
categories: [ML]
tags: [AIGC,ML,NLP]
usemath: latex
---

内容来自[Attention is all you need](https://arxiv.org/abs/1706.03762),该论文提出了自注意力机制与多头自注意力机制，所提出的Transformer架构在此基础上完全摒弃了以往的循环结构，而是完全依靠注意力机制，极大提高了训练效率和上下文捕捉能力。

## Self Attention



## Multi-Head Self Attention


## Transformer


