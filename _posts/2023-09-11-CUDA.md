---
title: CUDA编程
author: lonelywatch
date: 2023-09-11 20:31
categories: [CUDA]
tags: [CUDA]
---

## 介绍

GPU从开始的CPU协处理器变为现在的专用图形处理单元，其强大的并行计算能力（可以创建海量的线程）适用于**数据并行**的场景使用来加速计算。在cuda被正式创立前，只能通过将通用计算伪装成一个图形的渲染问题来得出结果，现在可以直接使用cuda来使用GPU计算资源。

## 简单的使用

在CUDA中CPU被称为**HOST**端，而GPU被称为**DEVICE**端。

使用CUDA很简单，因为GPU与CPU并不在同一个地址空间，所以需要显示的在GPU中分配内存，并且在GPU与CPU间复制转移数据。幸运的是，在CUDA 6.0之后增加了**Unified memory**，可以使用`cudaMallocManaged`来分配内存，在GPU端和CPU端都可以使用。（这主要是通过虚拟内存管理和缺页中断实现的）。并且在这种统一内存的作用下，使得CUDA编程能够与面对对象相结合。

在GPU中使用线程来执行相类似的计算，这个计算是一个被称为**kernel**的函数（在定义时和普通函数相比仅仅是增加了**修饰符**，然后使用时使用三个尖括号来表示配置参数，用来配置线程的维度和数量）。

并且需要注意的是，将kernel函数布置给GPU后，代码的控制权会立刻交给CPU，实现异步计算，如果需要等待计算完成，则需要使用`cudaDeviceSynchronize`来在计算完成前阻塞程序。

比方说一个向量加法：

```c++
#include <iostream>
#include <cmath>

__global__
void addVector(int n, float*x,float*y) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < n) {
        y[idx] = x[idx] + y[idx]; 
    }
}



int main() {

    int N = 1 << 20;
    float*x,*y;
    cudaMallocManaged(&x,N * sizeof(float));
    cudaMallocManaged(&y,N * sizeof(float));
    for (int i = 0; i < N; ++i) {
        x[i] = 1.0f;
        y[i] = 2.0f;
    }
    addVector<<<(N + 255) / 256,256>>>(N,x,y);
    cudaDeviceSynchronize();
    float maxError=0;
    for (int i = 0; i < N; ++i) {
        maxError = fmax(maxError,fabs(y[i] - 3.0f));
    }
    std::cout << "Max error is " << maxError;
    cudaFree(x);
    cudaFree(y);
    return 0;
}
```

上述代码使用`cudaMallocManaged`分配了两个1M float大小的内存，然后定义了一个kernel函数`addVector`。在使用时使用`<<<>>>`来设置配置参数。

然后使用synchronize来等待计算完成，最后使用`cudaFree`来释放内存。

---

对于kernel的配置参数，先得从CUDA中GPU多线程的逻辑结构说起，在CUDA中，是不能简单地指定线程的数量的。线程的逻辑结构为 ：使用若干`grid`网格，**grid**（网格）包含着若干**block**块，每个block包含若干线程，

其中grid和block可以是1，2，3维。每个维度的数量不能超过65535.

其中grid和block的维度通过`dim3`类型的对象来进行设置，使用标量赋值时维度缺省为1维。在kernel中，一些变量代表着这些值：`blockIdx`中有x，y，z三个分量，其中分别代表着3个维度的数量，blockIdx代表着每个block中每个维度上的线程数，`blockDim`则代表着网格三维上的block数量，`threadIdx`

则代表着thread在该块上三维的位置。

通过使用这三个对象来实现线程的定位，从而在特定的线程中执行特定的功能。

## 内存模型

每个SM中使用的内存有：`LOCAL`,`GLOBAL`,`CONSTANT`,`TEXTURE`，`SHARE`。对GLOBAL MEMORY的访问很大程度决定了CUDA的性能。

CUDA中还存在L1,和L2，CONSTANT,READ-ONLY缓存。其中L1为DRAM，通过行刷新，每行128bit。

CUDA中对于内存访问有着Memory transaction的概念。

当L1 disabled时，一次内存事务可以取得32，64，128bit，当L1 enabled时，就只能一次取得128bit。并且内存访问必须会对齐相应的内存大小，所以如果想要提高CUDA性能，就必须满足两个要求：

- 一组线程访问的地址尽量连续
- 地址要对齐

### 全局内存



### 共享内存与Bank Conflict

共享内存是片上内存，行列访问效率都很高，并且可以分成一个个可以同时访问的banks。

#### 使用



#### Bank Conflict



### 常量内存





## 调试

使用Nsight进行调试。

### 大作业

研究改进raytracing，真的不写不知道，写了才知道cuda的限制真的多。

- 不兼容c++ 标准库

所有的shared_ptr和vector这些都不能用，但是cuda好像有自己的stl库，有点麻烦。

- 调用的函数都必须是__device\_\_的。

很麻烦。

- 类中使用private时需要特别注意，不能使用cudaMemcpy。

如果一个对象包含若干private私有成员，即使它被复制到device设备上，也是不能访问的。（现象就是卡死，然后退出程序，显示访存错误）

- __constant\_\_不支持动态分配。 

真的太麻烦了。然后我之前使用glut还莫名其妙不能用x64的配置（印象中以前用的是x64,结果一查发现只支持x86）,然后去弄freeglut，还得必须是Debug版的，不然没有`freeglutd.lib`