---
title: python网络爬虫
author: lonelywatch
date: 2023-7-13 15:45 +0800
categories: [PYTHON]
tags: [PYTHON,WEB,爬虫]
---

## 烦人的东西

因为python课设要用到很多没学过的东西，所以只好先暂停编译，先来弄这个，虽然这个也挺有意思的，但是被强迫学习还是有点讨厌。

## 数据的存储

### CSV

爬虫需要将数据以一定方式进行存储，一种简单的方式是通过CSV(逗号分隔符)。该类型的文件内容将每行一个个数据以逗号分割，有若干行。通过python的csv库可以对csv文件进行相应操作。

```python
import csv

# 读csv文件
with open('example.csv', 'r') as file:
    # 创建读取器
    reader = csv.reader(file)
    for row in reader:
        print(row)
with open('example.csv', 'w') as file:
    writer = csv.writer(file)
    for xx in xxx:
        writer.writerow(xx,newline='')
```

通过创建读写器来进行读写，`writerow`写1行数据，`writerows`写多行数据，但数据必须是可迭代对象，`newline=''`避免产生空行。

对于线上的csv，可以选择下载到本地进行操作，也可以当作字符串将其封装为StringIO对象，直接加载进内存。

```python
#书上示例
from urllib.request import urlopen
from io import StringIO
import csv

data = urlopen('http://pythonscraping.com/files/MontyPythonAlbums.csv').read().decode('ascii','ignore')
dataFile = StringIO(data)
csvReader = csv.reader(dataFile)

for raw in csvReader:
    print(raw)

```



### 数据库

数据库用于存储大量数据，这里我使用开源免费的MYSQL，通过python中的PyMySQL库可以将python与MYSQL交互。MYSQL的安装及操作参考菜鸟教程。

通过连接到数据库来使用数据库，一个连接可以创建若干个光标，光标用于对数据库的操作。

```python
import pymysql

## 创建连接
with pymysql.connect(host='127.0.0.1', user='root',
                     password='ilikeyou2003', database='mysql') as conn:
    # 创建一个光标
    with conn.cursor() as cursor:
        cursor = conn.cursor()
        sql = 'SELECT * FROM user;'
        #执行语句
        cursor.execute(sql)
        #取得所有结果
        result = cursor.fetchall()

        for row in result:
            print(row)

```

## 爬虫

在python中可以使用urllib来进行url相关的操作，最常见的就是打开url`urlopen`(位于request子模块中)。同样`unquote`（parse子模块）可以将URL编码解码等等。re为python中的正则表达式库。BeautifulSoup是一个xml和html的解析器，bs4为它在python中的库。所谓爬虫，大致就是解析网页，在互联网的链接中爬取信息。

一个很简单的示例：

```python
from urllib.request import urlopen
from urllib.parse import unquote
from bs4 import BeautifulSoup
import re

srcUrl = 'https://lonelywatch.com'
html = urlopen(srcUrl)
bs = BeautifulSoup(html, 'html.parser')
for addr in bs.find_all('a', href=re.compile('/posts/.*')):
    print(unquote(addr.attrs['href']).lstrip('/posts/').rstrip('/'))

```

这个实例通过`urlopen`打开网页，使用`html.parser`创建BeautifulSoup对象，并通过其方法`find_all`找到该网页中所有`href`属性满足该正则表达式的`a`元素，并输出该标签对象中的 `href`排除了前后缀，达到了输出第一页中帖子名的功能。



### Scrapy



## 并行

Python支持多进程和多线程。Python的全局解释器锁可以阻止多个线程同时运行同一行代码，确保进程共享的内存不会中断。Python3版本使用`_thread`模块实现多线程。

### 多线程

书上的多线程爬虫示例：

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
import re
import random
import _thread
import time


def get_links(thread_name, bs):
    print('Getting links in {}'.format(thread_name))
    return bs.find('div', {'id': 'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))


def scrape_article(thread_name, path):
    html = urlopen('http://en.wikipedia.org{}'.format(path))
    time.sleep(5)
    bs = BeautifulSoup(html, 'html.parser')
    title = bs.find('h1').get_text()
    print('Scraping {} in thread {}.'.format(title,thread_name))
    links = get_links(thread_name,bs)
    if len(links) > 0:
        newArticle = links[random.randint(0,len(links) - 1)].attrs['href']
        print(newArticle)
        scrape_article(thread_name,newArticle)

try:
    _thread.start_new_thread(scrape_article, ('Thread 1', '/wiki/Kevin_Bacon',))
    _thread.start_new_thread(scrape_article, ('Thread 2', '/wiki/Monty_Python',))
except:
    print('Error: unable to start threads')

while 1:
    pass
```

通过创建两个线程分别从开始网页随机地爬取文章。

### 线程安全

可以使用锁来确保线程安全：

```python
threading.lock , threading.release
```

更多的是使用队列以线程安全的方式来传送静态数据。



### _thread和threading

_thread相对比较底层，threading则是线程的高级接口，可以通过`enumerate`静态函数来获取所有活跃现成的列表，`activeCount`获取总线程数。`currentThread`可以获取当前线程名称，`isAlive`判断线程是否存活，`isActive`判断是否活跃。threading的另一个优点就是可以创建其他线程无法访问的线程局部数据（通过`threading.local()`）

### 多进程

python中使用Process模块创建进程

```python
from multiprocessing import Process

t = Process(xx,args=(xx))
t.start()
t.join()
```

---

---

---

Fucking everything

### selenium

Selenium是一个用于自动化浏览器操作的开源工具和框架。它允许开发者使用多种编程语言（如Python、Java、C#等）来编写脚本，通过控制浏览器自动模拟用户在网页上的交互行为。它支持很多浏览器驱动（需要额外安装），这里使用edge来爬取b站的热门视频。

```python
import re
import pymysql
from selenium import webdriver
from bs4 import BeautifulSoup
import time

with pymysql.connect(host='127.0.0.1', user='root', password='xx', database='mysql') as conn:
    with conn.cursor() as cur:
        cur.execute('USE test1;')
        bilidriver = webdriver.ChromiumEdge()
        videoDriver = webdriver.ChromiumEdge()
        bilidriver.get('https://search.bilibili.com/all?vt=32443518&keyword=%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86')
        bs = BeautifulSoup(bilidriver.page_source, 'html.parser')
        for searchCard in bs.find_all('div', {'class': 'bili-video-card'}):
            addr = searchCard.find('a', {'data-mod': 'search-card'}).attrs['href'].lstrip('//')
            name = re.sub(r'\s*', '', searchCard.find('h3', {'class': 'bili-video-card__info--tit'}).attrs['title'])
            dura = searchCard.find('span', {'class': 'bili-video-card__stats__duration'}).get_text()
            videoDriver.get('https://{}'.format(addr))
            time.sleep(3)
            print('get video for https://{}'.format(addr))
            videoBs = BeautifulSoup(videoDriver.page_source, 'html.parser')
            spans = videoBs.find_all('span', {'class': re.compile('(view item)|(total-reply)')})
            plays = re.sub(r'\s', '', spans[0].get_text())
            comments = spans[1].get_text()
            print(name, addr, dura, plays, comments)
            cur.execute(
                'INSERT INTO t1(className,platform,plays,comments,duration) VALUES(\'{}\',\'bilibili\',\'{}\',{},\'{}\')'.format(
                    name, plays, comments, dura))
            for line in cur.fetchall():
                print(line)
        videoDriver.close()
        bilidriver.close()
        cur.connection.commit()

```

上述测试示例可以爬取当日b站的热门视频，获取播放地址，视频名以及播放数,将其存入测试的数据库。

